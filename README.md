Auto Regressive (AR) models use tokens generated in previous time-step as input to calculate outputs. The Transformers are by-and-large autoregressive in nature. Coupled with an attention mechanism, an auto regressive transformer model gets the right context thereby increasing its accuracy. In contrast, Non-Auto Regressive (NAR) models generate a sequence of tokens in parallel removing the reliance on the tokens from previous time-steps. This approach significantly reduces the inference latency of the output. However, at the expense of low accuracy. In this project, we explore the implementations of both these approaches and assess options to narrow the accuracy gap. We changed the architecture of the decoder as part of the NAR implementation and introduced Conditional Random Fields (CRF) to generate the output sequence of a machine translation task. We used IWSLT dataset for German (DE) to English (EN) translation task. We were able to train both the models from scratch on a GPU based server and we observed BLEU scores of AR & NAR to be 16.07 and 8.79 respectively.

